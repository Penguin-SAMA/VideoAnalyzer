# 研究论文报告

## From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding

>   链接：https://ar5iv.labs.arxiv.org/html/2409.18938

主要讲了如何将 LLM 与特定视觉模态编码器进行结合，赋予 LLM 视觉感知能力。

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2409.18938/assets/x2.png)

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2409.18938/assets/x3.png)

![Refer to caption](https://ar5iv.labs.arxiv.org/html/2409.18938/assets/x4.png)

## Video-Language Understanding: A Survey from Model Architecture, Model Training, and Data Perspectives 

>   link: https://arxiv.org/abs/2406.05615

相关性不强，只讲了各种视频处理的区别。

![Refer to caption](https://arxiv.org/html/2406.05615v3/x1.png)

## ⭐Video Token Merging for Long-form Video Understanding

>   link: https://papers.nips.cc/paper/2024/hash/194fa4536bf36f35a4505d20cd5dd6fc-Abstract-Conference.html

做token merge。

![image-20251021155919679](./%E7%A0%94%E7%A9%B6%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A.assets/image-20251021155919679.png)

方案：

1.   Naive VTM：在 Transformer 中插入 VTM block。会忽略掉重要 token。
2.   Region-concentrated VTM：50% 的目标 token 从画面中心抽样。会忽略视频边缘。
3.   Motion-based VTM：利用运动信息作权重，认为“运动信更明显”的 token 更重要。
4.   Learnable VTM：训练一个模块来估计每个 token 的重要程度，基于 saliency 分数来决定保留/合并哪些 token。
     -   主路径处理 Transformer self-attention，辅路径估计 token 的 saliency 分数。
     -   作者：在内存消耗上降低了 84%，吞吐量提高 6.89x。

整体流程：

-   在每一 VTM block（插入于 Transformer 层）中：
    1.  对当前 token 集 $X$ 计算 saliency（learnable 路径）
    2.  Partition $X$→$T$（target）+$S$（source）依据 saliency／策略
    3.  对每 $x_j \in S$ 找匹配 $x_i \in T$（如相似度最大）
    4.  合并 $x_j$ 到 $x_i$，生成新的 token set $Y$（|Y| < |X|）
    5.  下一 Transformer 层以 $Y$ 为输入。

## Classroom Behavior Recognition Using Computer Vision: A Comprehensive Review（Sensors, 2025）

>   link; https://www.mdpi.com/1424-8220/25/2/373

本文针对基于计算机视觉的**课堂行为**识别问题开展系统性综述。行为计算基于视觉线索，能够大规模、实时地捕捉课堂中教师与学生状态。但当前在“使用计算机视觉识别课堂行为”的研究中，尚缺乏对整体研究现状、目标分类、识别技术、未来趋势的统一共识。

## ⭐From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding

>   link：https://github.com/Vincent-ZHQ/Comprehensive-Long-Video-Understanding-Survey
>   paper link：https://arxiv.org/pdf/2409.18938
>
>   

对小时级视频进行处理，讲解了多个大模型的处理能力，有不同模型的基准测试表单。

重点模型：**NVILA**，**LONGVILA**，**TimeMarker**

## Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding

>   link：https://arxiv.org/abs/2403.16276

论文目标：让多模态大模型具备时间感知力，把音视频事件，文本描述，时间区间对齐，完成**时间定位**+**对话式问答**。